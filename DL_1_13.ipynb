{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basantiroomie/ML/blob/main/DL_1_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1\n",
        "import numpy as np\n",
        "\n",
        "def hebbian_learning_rule(input_pattern, weight_matrix):\n",
        "    return weight_matrix + np.outer(input_pattern, input_pattern)\n",
        "\n",
        "def perceptron_learning_rule(input_pattern, target, weight_vector, learning_rate):\n",
        "    prediction = np.dot(weight_vector, input_pattern)\n",
        "    error = target - prediction\n",
        "    return weight_vector + learning_rate * error * input_pattern\n",
        "\n",
        "def delta_learning_rule(input_pattern, target, weight_matrix, learning_rate):\n",
        "    prediction = np.dot(weight_matrix, input_pattern)\n",
        "    error = target - prediction\n",
        "    return weight_matrix + learning_rate * np.outer(error, input_pattern)\n",
        "\n",
        "def correlation_learning_rule(input_pattern, weight_matrix):\n",
        "    return weight_matrix + np.outer(input_pattern, input_pattern)\n",
        "\n",
        "def out_star_learning_rule(input_pattern, weight_matrix, learning_rate):\n",
        "    return weight_matrix + learning_rate * np.outer(input_pattern, input_pattern)\n",
        "\n",
        "input_size=3\n",
        "hebbian_weights = np.random.rand(input_size, input_size)\n",
        "perceptron_weights=np.random.rand(input_size)\n",
        "delta_weights=np.random.rand(input_size,input_size)\n",
        "correlation_weights=np.random.rand(input_size,input_size)\n",
        "out_star_weights=np.random.rand(input_size,input_size)\n",
        "\n",
        "print(\"Hebbian Weights:\",hebbian_weights)\n",
        "print(\"\\nPerceptron Weights:\",perceptron_weights)\n",
        "print(\"\\nDelta Weights:\",delta_weights)\n",
        "print(\"\\nCorrelation Weights:\",correlation_weights)\n",
        "print(\"\\nOut Star Weights:\",out_star_weights)\n",
        "\n",
        "input_pattern = np.array([0.2,0.5,0.8])\n",
        "target=1\n",
        "\n",
        "hebbian_weights_updated = hebbian_learning_rule(input_pattern, hebbian_weights)\n",
        "\n",
        "perceptron_weights_updated = perceptron_learning_rule(input_pattern, target, perceptron_weights, learning_rate=0.1)\n",
        "\n",
        "delta_weights_updated = delta_learning_rule(input_pattern, target, delta_weights, learning_rate=0.1)\n",
        "\n",
        "correlation_weights_updated = correlation_learning_rule(input_pattern, correlation_weights)\n",
        "\n",
        "out_star_weights_updated = out_star_learning_rule(input_pattern, out_star_weights, learning_rate=0.1)\n",
        "\n",
        "print(\"Hebbian Updated Weights:\", hebbian_weights_updated)\n",
        "print(\"\\nPerceptron Updated Weights:\", perceptron_weights_updated)\n",
        "print(\"\\nDelta Updated Weights:\", delta_weights_updated)\n",
        "print(\"\\nCorrelation Update Weights:\", correlation_weights_updated)\n",
        "print(\"\\nOut Star Updated Weights:\", out_star_weights_updated)"
      ],
      "metadata": {
        "id": "U33AAs9Qk-eH"
      },
      "id": "U33AAs9Qk-eH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_sigmoid():\n",
        "    x = np.linspace(-10,10,100)\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    plt.plot(x,y)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Sigmoid Output\")\n",
        "    plt.title(\"Sigmoid Activation Function\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_tanh():\n",
        "    x = np.linspace(-10,10,100)\n",
        "    tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    plt.plot(x,tanh)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Hyperbolic Tangent Output\")\n",
        "    plt.title(\"Hyperbolic Tangent Function\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_ReLU():\n",
        "    x = np.linspace(-10,10,100)\n",
        "    ReLU = np.maximum(0,x)\n",
        "    plt.plot(x,ReLU)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"ReLU Output\")\n",
        "    plt.title(\"Rectified Linear Unit\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_LeakyReLU(alpha=0.01):\n",
        "    x = np.linspace(-10,10,100)\n",
        "    LeakyReLU = np.where(x > 0, x, alpha * x)\n",
        "    plt.plot(x,LeakyReLU)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"LeakyReLU Output\")\n",
        "    plt.title(\"Leaky ReLU\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_ELU(alpha=0.01):\n",
        "    x = np.linspace(-10,10,100)\n",
        "    ELU = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "    plt.plot(x,ELU)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"ELU Output\")\n",
        "    plt.title(\"Exponential Linear Unit\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_swish(beta=1):\n",
        "    x = np.linspace(-10,10,100)\n",
        "    swish = x * 1 / (1 + np.exp(-(beta * x)))\n",
        "    plt.plot(x,swish)\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Swish Output\")\n",
        "    plt.title(\"Swish\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_sigmoid()\n",
        "plot_tanh()\n",
        "plot_ReLU()\n",
        "plot_LeakyReLU()\n",
        "plot_ELU()\n",
        "plot_swish()\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x)) # Stability trick\n",
        "    return exp_x / exp_x.sum(axis=0)\n",
        "\n",
        "x = np.linspace(-10,10,100)\n",
        "softmax_values = softmax(np.array([x, x/2, x/3]))\n",
        "#plt.subplot(3, 3, 7)\n",
        "for i in range(softmax_values.shape[0]):\n",
        "    plt.plot(x,softmax_values[i], label = f\"Softmax Output {i+1}\")\n",
        "\n",
        "plt.title(\"Softmax Activation Function\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hy5oqTzflAAp"
      },
      "id": "hy5oqTzflAAp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import numpy as np\n",
        "\n",
        "def step_function(x):\n",
        "    return 1 if x>=0 else 0\n",
        "\n",
        "def perceptron_decision(inputs,weights, bias):\n",
        "    weighted_sum=np.dot(inputs,weights)+bias\n",
        "    return step_function(weighted_sum)\n",
        "\n",
        "dataset = [\n",
        "    {\"inputs\": [1,0,1], \"expected\": 1},\n",
        "    {\"inputs\": [1,1,0], \"expected\": 0},\n",
        "    {\"inputs\": [0,0,1], \"expected\":1},\n",
        "    {\"inputs\": [1,1,1], \"expected\":1},\n",
        "    {\"inputs\": [0,1,0], \"expected\":0}\n",
        "]\n",
        "\n",
        "weights = np.array([0.2, 0.4, 0.2])\n",
        "bias = -0.5\n",
        "correct_predictions=0\n",
        "total_samples=len(dataset)\n",
        "print(\"Inputs\\t\\tPrediction\\tExpected\\tResult\")\n",
        "for data in dataset:\n",
        "    inputs=np.array(data[\"inputs\"])\n",
        "    expected_output = data[\"expected\"]\n",
        "\n",
        "    predicted_output = perceptron_decision(inputs,weights,bias)\n",
        "\n",
        "    if predicted_output == expected_output:\n",
        "        correct_predictions +=1\n",
        "\n",
        "    result = \"Correct\" if predicted_output == expected_output else \"Incorrect\"\n",
        "    print(f\"{inputs}\\t\\t{predicted_output}\\t\\t{expected_output}\\t\\t{result}\")\n",
        "\n",
        "accuracy = (correct_predictions / total_samples) * 100\n",
        "print(f\"\\nModel Accuracy: {accuracy: 2f}%\")"
      ],
      "metadata": {
        "id": "-cJLdw7TlGcW"
      },
      "id": "-cJLdw7TlGcW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "img1=cv2.imread('/content/bright.jpeg')\n",
        "plt.axis('off')\n",
        "plt.title('Bright image')\n",
        "plt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "img2=cv2.imread('/content/dark.jpeg')\n",
        "plt.axis('off')\n",
        "plt.title('Dark image')\n",
        "plt.imshow(cv2.cvtColor(img2,cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "img3=cv2.imread('/content/medium.jpeg')\n",
        "plt.axis('off')\n",
        "plt.title('Medium image')\n",
        "plt.imshow(cv2.cvtColor(img3,cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "histogram=cv2.calcHist([img1],[0],None,[256],[0,256])\n",
        "plt.plot(histogram,color='black')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "colors=['b','g','r']\n",
        "for color in colors:\n",
        "  hist=cv2.calcHist([img1],[i],None,[256],[0,256])\n",
        "  plt.plot(hist,color=color)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "gray_image=cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY)\n",
        "plt.imshow(gray_image)\n",
        "plt.title('gray image')\n",
        "plt.show()\n",
        "\n",
        "resized=cv2.resize(src=img1,dsize=(970,220),interpolation=cv2.INTER_CUBIC)\n",
        "plt.imshow(resized)\n",
        "plt.show()\n",
        "\n",
        "gaussian=cv2.GaussianBlur(resized,(15,5),0)\n",
        "plt.imshow(gaussian)\n",
        "plt.show()\n",
        "\n",
        "canny=cv2.Canny(resized,100,200)\n",
        "plt.imshow(canny)\n",
        "plt.title('Canny')\n",
        "plt.show()\n",
        "\n",
        "brighted=cv2.addWeighted(resized,1.2,resized,0,70)\n",
        "plt.imshow(brighted)\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n",
        "\n",
        "datagen=ImageDataGenerator(rotation_range=20,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,vertical_flip=True,fill_mode='nearest')\n",
        "img=load_img('/content/dark.jpeg')\n",
        "x=img_to_array(img)\n",
        "x=np.expand_dims(x,axis=0)\n",
        "\n",
        "num_images=5\n",
        "augmented_images=[]\n",
        "for i,batch in enumerate(datagen.flow(x,batch_size=1)):\n",
        "  augmented_images.append(array_to_img(batch[0]))\n",
        "  if i>=num_images-1:\n",
        "    break\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,num_images+1,1)\n",
        "plt.imshow(img)\n",
        "plt.title('Original')\n",
        "\n",
        "for i in range(len(augmented_images)):\n",
        "  plt.subplot(1,num_images+1,i+1+1)\n",
        "  plt.imshow(augmented_images[i])\n",
        "  plt.title('Augmented')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from skimage import io,exposure\n",
        "img=io.imread('/content/dark.jpeg',as_gray=True)\n",
        "equalized=exposure.equalize_hist(img)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title('Input image')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(equalized,cmap='gray')\n",
        "plt.title('Equalized image')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "binary_img=cv2.imread('/content/bright.jpeg',cv2.IMREAD_GRAYSCALE)\n",
        "_,binary_img=cv2.threshold(binary_img,127,255,cv2.THRESH_BINARY)\n",
        "kernel=np.ones((5,5),np.uint8)\n",
        "eroded_image=cv2.erode(binary_img,kernel,iterations=1)\n",
        "dilated_image=cv2.dilate(binary_img,kernel,iterations=1)\n",
        "opening=cv2.morphologyEx(binary_img,cv2.MORPH_OPEN,kernel)\n",
        "closing=cv2.morphologyEx(binary_img,cv2.MORPH_CLOSE,kernel)\n",
        "gradient=cv2.morphologyEx(binary_img,cv2.MORPH_GRADIENT,kernel)\n",
        "\n",
        "titles=['Original','Erosion','Dilation','Opening','Closing','Gradient']\n",
        "images=[binary_img,eroded_image,dilated_image,opening,closing,gradient]\n",
        "for i in range(len(images)):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(images[i],cmap='gray')\n",
        "  plt.title(titles[i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GmjdkicllMOG"
      },
      "id": "GmjdkicllMOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "!curl -o style.jpg https://i.imgur.com/9ooB60I.jpg\n",
        "!curl -o content.jpg https://i.imgur.com/F28w3Ac.jpg\n",
        "\n",
        "def load_img(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Could not read image at path: {path}\")\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "content_image = load_img('content.jpg')\n",
        "style_image = load_img('style.jpg')\n",
        "\n",
        "model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "\n",
        "def apply_style(content_image, style_image):\n",
        "    content_image = content_image.reshape(1, *content_image.shape).astype('float32')\n",
        "    style_image = cv2.resize(style_image, (256, 256))\n",
        "    style_image = style_image.reshape(1, *style_image.shape).astype('float32')\n",
        "    outputs = model(tf.constant(content_image), tf.constant(style_image))\n",
        "    stylized_image = outputs[0]\n",
        "    return stylized_image\n",
        "\n",
        "stylized_img = apply_style(content_image, style_image)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(stylized_img[0])\n",
        "plt.title(\"Stylized Image\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GCamTp_DlU6u"
      },
      "id": "GCamTp_DlU6u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "cifar_10_classes = [\n",
        "    \"Airplane\",\n",
        "    \"Automobile\",\n",
        "    \"Bird\",\n",
        "    \"Cat\",\n",
        "    \"Deer\",\n",
        "    \"Dog\",\n",
        "    \"Frog\",\n",
        "    \"Horse\",\n",
        "    \"Ship\",\n",
        "    \"Truck\"\n",
        "]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x_train[0])\n",
        "plt.title(cifar_10_classes[y_train[0][0]])\n",
        "plt.axis(\"off\")\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "y_train = one_hot_encoder.fit_transform(y_train).toarray()\n",
        "y_test = one_hot_encoder.transform(y_test).toarray()"
      ],
      "metadata": {
        "id": "orgi9i4PlePM"
      },
      "id": "orgi9i4PlePM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da8d6d6-237a-4a5a-bc95-30a575e73569",
      "metadata": {
        "id": "1da8d6d6-237a-4a5a-bc95-30a575e73569"
      },
      "outputs": [],
      "source": [
        "#7\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "dataset = keras.datasets.mnist\n",
        "\n",
        "class_names = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
        "(x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
        "x_train=x_train.reshape((x_train.shape[0], x_train.shape[1],x_train.shape[2],1))\n",
        "x_test=x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
        "print(x_train.shape)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.title(class_names[y_train[i]])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255\n",
        "\n",
        "model =keras.models.Sequential([\n",
        "    keras.layers.Conv2D(64, (3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
        "    keras.layers.Conv2D (64, (3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
        "model.fit(x_train,y_train, epochs=5, callbacks=keras.callbacks.EarlyStopping(patience=2))\n",
        "model.evaluate(x_test,y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce503eb",
      "metadata": {
        "id": "0ce503eb"
      },
      "outputs": [],
      "source": [
        "#8\n",
        "# sonar dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "df = pd.read_csv(\"sonar_dataset.csv\", header = None)\n",
        "\n",
        "X = df.drop(60, axis = 1)\n",
        "y = df[60]\n",
        "y = pd.get_dummies(y, drop_first = True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
        "    keras.layers.Dense(30, activation='relu'),\n",
        "    keras.layers.Dense(15, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=8)\n",
        "model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_test).reshape(-1)\n",
        "y_pred = np.round(y_pred)\n",
        "print(y_pred[:10])\n",
        "print(y_test[:10])\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee3e58e",
      "metadata": {
        "id": "4ee3e58e"
      },
      "outputs": [],
      "source": [
        "# 9\n",
        "# Title:Study the effect of batch normalization and dropout in neural network classifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization,Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "model = keras.Sequential([\n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(128, activation='relu'),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.2),\n",
        "  Dense(64, activation='relu'),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.2),\n",
        "  Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))\n",
        "\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Train accuracy:\", train_acc)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a3d4d1",
      "metadata": {
        "id": "53a3d4d1"
      },
      "outputs": [],
      "source": [
        "# 10\n",
        "# Sentiment Analysis of IMDb Movie Reviews using GRU-based Neural Network\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 32\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GRU, Dense, Dropout\n",
        "model = Sequential([\n",
        " Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        " GRU(64, return_sequences=True),\n",
        " GRU(32),\n",
        " Dense(16, activation='relu'),\n",
        " Dropout(0.5),\n",
        " Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64,\n",
        "validation_split=0.2)\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "import numpy as np\n",
        "def predict_sentiment(text):\n",
        " text_sequence = imdb.get_word_index()\n",
        " tokens = [text_sequence.get(word, 0) for word in text.lower().split()]\n",
        " tokens_padded = pad_sequences([tokens], maxlen=max_length, padding='post', truncating='post')\n",
        " prediction = model.predict(tokens_padded)\n",
        " sentiment = 'positive' if prediction >= 0.5 else 'negative'\n",
        " return sentiment\n",
        "new_review = \"The movie was fantastic and I loved it\"\n",
        "print(f'Sentiment: {predict_sentiment(new_review)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc24565d",
      "metadata": {
        "id": "dc24565d"
      },
      "outputs": [],
      "source": [
        "# 11\n",
        "# Experiment No: 12 Next Word Prediction Using an RNN on Simple English Sentences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    \"I love to eat apples\",\n",
        "    \"She loves to eat oranges\",\n",
        "    \"He likes to eat bananas\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "seq_length = 3\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(seq_length, len(seq)):\n",
        "        input_sequences.append(seq[i-seq_length:i])\n",
        "        output_words.append(seq[i])\n",
        "\n",
        "X = pad_sequences(input_sequences, maxlen=seq_length, padding='pre')\n",
        "y = tf.keras.utils.to_categorical(output_words, num_classes=vocab_size)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=seq_length))\n",
        "model.add(SimpleRNN(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, seq_length):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=seq_length, padding='pre')\n",
        "    predicted = np.argmax(model.predict(sequence), axis=-1)\n",
        "    return tokenizer.index_word[predicted[0]]\n",
        "\n",
        "input_text = \"I love to\"\n",
        "next_word = predict_next_word(model, tokenizer, input_text, seq_length)\n",
        "print(f\"Input text: '{input_text}', Predicted next word: '{next_word}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70b8770",
      "metadata": {
        "id": "c70b8770"
      },
      "outputs": [],
      "source": [
        "# 12\n",
        "# Experiment No: 13 Conversational Chatbot using Bidirectional LSTM\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "conversations = [\n",
        "    (\"Hi\", \"Hello!\"),\n",
        "    (\"How are you?\", \"I'm good, thank you!\"),\n",
        "    (\"What's your name?\", \"I'm a chatbot.\"),\n",
        "    (\"What do you do?\", \"I talk to people.\"),\n",
        "    (\"Bye\", \"Goodbye!\")\n",
        "]\n",
        "\n",
        "inputs, outputs = zip(*conversations)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(inputs + outputs)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "input_seqs = tokenizer.texts_to_sequences(inputs)\n",
        "output_seqs = tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "max_len = max(max(len(seq) for seq in input_seqs), max(len(seq) for seq in output_seqs))\n",
        "X = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
        "y = pad_sequences(output_seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 64, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "def generate_response(model, tokenizer, user_input, max_len):\n",
        "    seq = tokenizer.texts_to_sequences([user_input])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded)\n",
        "    pred_seq = np.argmax(pred, axis=-1)[0]\n",
        "    words = [tokenizer.index_word.get(i, '') for i in pred_seq]\n",
        "    response = ' '.join(word for word in words if word != '')\n",
        "    return response.strip()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "    response = generate_response(model, tokenizer, user_input, max_len)\n",
        "    print(\"Bot:\", response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}