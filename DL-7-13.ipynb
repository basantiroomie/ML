{
 "cells": [
{
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8d6d6-237a-4a5a-bc95-30a575e73569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "dataset = keras.datasets.mnist\n",
    "\n",
    "class_names = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
    "(x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
    "x_train=x_train.reshape((x_train.shape[0], x_train.shape[1],x_train.shape[2],1))\n",
    "x_test=x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
    "print(x_train.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(class_names[y_train[i]])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "model =keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
    "    keras.layers.Conv2D (64, (3,3), input_shape=(28,28,1), activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=(2,2), strides=1),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense (64, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,y_train, epochs=5, callbacks=keras.callbacks.EarlyStopping(patience=2))\n",
    "\n",
    "model.evaluate(x_test,y_test)\n",
    "\n",
    "sample_img = x_test[0]\n",
    "sample_img.shape\n",
    "plt.imshow(sample_img)\n",
    "\n",
    "img = np.expand_dims(sample_img,axis=0)\n",
    "img.shape\n",
    "\n",
    "pred = model.predict(img)\n",
    "\n",
    "pred\n",
    "\n",
    "print(f\"Predicted; {class_names[np.argmax(pred)]}\\nActual: {class_names[y_test[0]]}\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce503eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "# sonar dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv(\"sonar_dataset.csv\", header = None)\n",
    "df.sample(5)\n",
    "X = df.drop(60, axis = 1)\n",
    "y = df[60]\n",
    "y = pd.get_dummies(y, drop_first = True)\n",
    "y.value_counts()\n",
    "X.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "X_train.head()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(60,\n",
    "                       input_dim=60,\n",
    "                       activation='relu'),\n",
    "    keras.layers.Dense(30,\n",
    "                       activation='relu'),\n",
    "    keras.layers.Dense(15,\n",
    "                       activation='relu'),\n",
    "    keras.layers.Dense(1,\n",
    "                       activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8)\n",
    "model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test).reshape(-1)\n",
    "y_pred = np.round(y_pred)\n",
    "print(y_pred[:10])\n",
    "\n",
    "y_test[:10]\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "# Title:Study the effect of batch normalization and dropout in neural network classifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "model = Sequential([\n",
    " Flatten(input_shape=(28, 28)),\n",
    " Dense(128, activation='relu'),\n",
    " BatchNormalization(),\n",
    " Dropout(0.2),\n",
    " Dense(64, activation='relu'),\n",
    " BatchNormalization(),\n",
    " Dropout(0.2),\n",
    " Dense(10, activation='softmax')\n",
    " ])\n",
    "model.compile(optimizer=\"adam\",\n",
    " loss='sparse_categorical_crossentropy',\n",
    " metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
    "validation_data=(X_test, y_test))\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "# Sentiment Analysis of IMDb Movie Reviews using GRU-based Neural Network\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "embedding_dim = 32\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense, Dropout\n",
    "model = Sequential([\n",
    " Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    " GRU(64, return_sequences=True),\n",
    " GRU(32),\n",
    " Dense(16, activation='relu'),\n",
    " Dropout(0.5),\n",
    " Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "validation_split=0.2)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "import numpy as np\n",
    "def predict_sentiment(text):\n",
    " text_sequence = imdb.get_word_index()\n",
    " tokens = [text_sequence.get(word, 0) for word in text.lower().split()]\n",
    " tokens_padded = pad_sequences([tokens], maxlen=max_length, padding='post', truncating='post')\n",
    " prediction = model.predict(tokens_padded)\n",
    " sentiment = 'positive' if prediction >= 0.5 else 'negative'\n",
    " return sentiment\n",
    "new_review = \"The movie was fantastic and I loved it\"\n",
    "print(f'Sentiment: {predict_sentiment(new_review)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# Experiment No: 12 Next Word Prediction Using an RNN on Simple English Sentences\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    \"I love to eat apples\",\n",
    "    \"She loves to eat oranges\",\n",
    "    \"He likes to eat bananas\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "input_sequences = []\n",
    "output_words = []\n",
    "seq_length = 3\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(seq_length, len(seq)):\n",
    "        input_sequences.append(seq[i-seq_length:i])\n",
    "        output_words.append(seq[i])\n",
    "\n",
    "X = pad_sequences(input_sequences, maxlen=seq_length, padding='pre')\n",
    "y = tf.keras.utils.to_categorical(output_words, num_classes=vocab_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=seq_length))\n",
    "model.add(SimpleRNN(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "def predict_next_word(model, tokenizer, text, seq_length):\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = pad_sequences([sequence], maxlen=seq_length, padding='pre')\n",
    "    predicted = np.argmax(model.predict(sequence), axis=-1)\n",
    "    return tokenizer.index_word[predicted[0]]\n",
    "\n",
    "input_text = \"I love to\"\n",
    "next_word = predict_next_word(model, tokenizer, input_text, seq_length)\n",
    "print(f\"Input text: '{input_text}', Predicted next word: '{next_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "# Experiment No: 13 Conversational Chatbot using Bidirectional LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "conversations = [\n",
    "    (\"Hi\", \"Hello!\"),\n",
    "    (\"How are you?\", \"I'm good, thank you!\"),\n",
    "    (\"What's your name?\", \"I'm a chatbot.\"),\n",
    "    (\"What do you do?\", \"I talk to people.\"),\n",
    "    (\"Bye\", \"Goodbye!\")\n",
    "]\n",
    "\n",
    "inputs, outputs = zip(*conversations)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(inputs + outputs)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "input_seqs = tokenizer.texts_to_sequences(inputs)\n",
    "output_seqs = tokenizer.texts_to_sequences(outputs)\n",
    "\n",
    "max_len = max(max(len(seq) for seq in input_seqs), max(len(seq) for seq in output_seqs))\n",
    "X = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
    "y = pad_sequences(output_seqs, maxlen=max_len, padding='post')\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "def generate_response(model, tokenizer, user_input, max_len):\n",
    "    seq = tokenizer.texts_to_sequences([user_input])\n",
    "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    pred = model.predict(padded)\n",
    "    pred_seq = np.argmax(pred, axis=-1)[0]\n",
    "    words = [tokenizer.index_word.get(i, '') for i in pred_seq]\n",
    "    response = ' '.join(word for word in words if word != '')\n",
    "    return response.strip()\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    response = generate_response(model, tokenizer, user_input, max_len)\n",
    "    print(\"Bot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
